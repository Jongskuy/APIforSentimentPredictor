{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a256f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca014f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_preprocess.tsv.txt', sep='\\t', names=['tweet','HS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536693cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(sent):\n",
    "    string = sent.lower()\n",
    "    string = re.sub(r'[^a-zA-Z0-9]', ' ', string)\n",
    "    string = re.sub(\"[,]\", \" ,\", string)\n",
    "    string = re.sub(\"[.]\", \" .\", string)\n",
    "    string = re.sub(\"[?]\", \" ? \", string)\n",
    "    string = re.sub(\"[!]\", \" !\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3673927",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_tokenizer = open('tokenizer.p', 'rb')\n",
    "tokenizer = pickle.load(open_tokenizer)\n",
    "\n",
    "x = open(\"x_pad_sequences.p\",'rb')\n",
    "X = pickle.load(x)\n",
    "\n",
    "y = open(\"y_labels.p\",'rb')\n",
    "Y = pickle.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1398fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df.tweet.apply(cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e36d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = df.loc[df['HS'] == 'negative'].text_clean.tolist()\n",
    "neu = df.loc[df['HS'] == 'neutral'].text_clean.tolist()\n",
    "pos = df.loc[df['HS'] == 'positive'].text_clean.tolist()\n",
    "\n",
    "neg_label = df.loc[df['HS'] == 'negative'].HS.tolist()\n",
    "neu_label = df.loc[df['HS'] == 'neutral'].HS.tolist()\n",
    "pos_label = df.loc[df['HS'] == 'positive'].HS.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b61e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: 6416, Neu: 1148, Neg: 3436\n",
      "Total data: 11000\n"
     ]
    }
   ],
   "source": [
    "total_data = pos + neu + neg\n",
    "labels = pos_label + neu_label + neg_label\n",
    "\n",
    "print(\"Pos: %s, Neu: %s, Neg: %s\" % (len(pos), len(neu), len(neg)))\n",
    "print(\"Total data: %s\" % len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95cb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48b94f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 96, 100)           10000000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 92, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,065,451\n",
      "Trainable params: 10,065,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "880/880 [==============================] - 167s 188ms/step - loss: 0.3363 - accuracy: 0.7576 - val_loss: 0.2194 - val_accuracy: 0.8655\n",
      "Epoch 2/10\n",
      "880/880 [==============================] - 160s 181ms/step - loss: 0.1276 - accuracy: 0.9306 - val_loss: 0.2072 - val_accuracy: 0.8759\n",
      "Epoch 2: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_dim = 100\n",
    "max_features = 100000\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "maxlen = max(len(x) for x in X)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_test, y_test), verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc09008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 6ms/step\n",
      "Testing selesai\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       704\n",
      "           1       0.79      0.82      0.80       222\n",
      "           2       0.91      0.92      0.92      1274\n",
      "\n",
      "    accuracy                           0.88      2200\n",
      "   macro avg       0.85      0.85      0.85      2200\n",
      "weighted avg       0.88      0.88      0.88      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions = model.predict(X_test)\n",
    "# y_pred = predictions\n",
    "# matrix_test = metrics.classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "# print(\"Testing selesai\")\n",
    "# print(matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1f6cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 2s 13ms/step\n",
      "Training ke- 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       704\n",
      "           1       0.82      0.76      0.79       222\n",
      "           2       0.90      0.94      0.92      1274\n",
      "\n",
      "    accuracy                           0.88      2200\n",
      "   macro avg       0.86      0.83      0.85      2200\n",
      "weighted avg       0.88      0.88      0.88      2200\n",
      "\n",
      "======================================================\n",
      "69/69 [==============================] - 1s 6ms/step\n",
      "Training ke- 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       704\n",
      "           1       0.81      0.79      0.80       222\n",
      "           2       0.91      0.93      0.92      1274\n",
      "\n",
      "    accuracy                           0.88      2200\n",
      "   macro avg       0.85      0.84      0.85      2200\n",
      "weighted avg       0.88      0.88      0.88      2200\n",
      "\n",
      "======================================================\n",
      "69/69 [==============================] - 2s 19ms/step\n",
      "Training ke- 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       704\n",
      "           1       0.79      0.80      0.80       222\n",
      "           2       0.92      0.92      0.92      1274\n",
      "\n",
      "    accuracy                           0.88      2200\n",
      "   macro avg       0.85      0.85      0.85      2200\n",
      "weighted avg       0.88      0.88      0.88      2200\n",
      "\n",
      "======================================================\n",
      "69/69 [==============================] - 1s 11ms/step\n",
      "Training ke- 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84       704\n",
      "           1       0.84      0.75      0.79       222\n",
      "           2       0.92      0.93      0.92      1274\n",
      "\n",
      "    accuracy                           0.88      2200\n",
      "   macro avg       0.86      0.84      0.85      2200\n",
      "weighted avg       0.88      0.88      0.88      2200\n",
      "\n",
      "======================================================\n",
      "69/69 [==============================] - 2s 13ms/step\n",
      "Training ke- 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81       704\n",
      "           1       0.94      0.49      0.64       222\n",
      "           2       0.89      0.93      0.91      1274\n",
      "\n",
      "    accuracy                           0.86      2200\n",
      "   macro avg       0.87      0.75      0.79      2200\n",
      "weighted avg       0.86      0.86      0.85      2200\n",
      "\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "Rata-rata Accuracy:  0.875909090909091\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Percobaan 5\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = Y\n",
    "\n",
    "embed_dim = 100\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "\n",
    "    data_train   = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "\n",
    "    data_test    = X[data[1]]\n",
    "    target_test  = y[data[1]]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0)\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_test, y_test), verbose=0, callbacks=[es])\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = predictions\n",
    "    model.save(\"model_CNN{}.h5\".format(iteration))\n",
    "\n",
    "    # for the current fold only    \n",
    "    accuracy = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    print(\"Training ke-\", iteration)\n",
    "    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Rata-rata Accuracy: \", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e739abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 179ms/step\n",
      "Text:   rasa syukur  cukup  \n",
      "Sentiment:  positive\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from keras.models import load_model\n",
    "\n",
    "input_text = \"\"\"\n",
    "Rasa syukur, cukup.\n",
    "\"\"\"\n",
    "\n",
    "def cleansing(sent):\n",
    "    # Mengubah kata menjadi huruf kecil semua dengan menggunakan fungsi lower()\n",
    "    string = sent.lower()\n",
    "    # Menghapus emoticon dan tanda baca menggunakan \"RegEx\" dengan script di bawah\n",
    "    string = re.sub(r'[^a-zA-Z0-9]', ' ', string)\n",
    "    return string\n",
    "\n",
    "sentiment = ['negative', 'neutral', 'positive']\n",
    "\n",
    "text = [cleansing(input_text)]\n",
    "predicted = tokenizer.texts_to_sequences(text)\n",
    "guess = pad_sequences(predicted, maxlen=X.shape[1])\n",
    "\n",
    "model = load_model('model_CNN5.h5')\n",
    "prediction = model.predict(guess)\n",
    "polarity = np.argmax(prediction[0])\n",
    "\n",
    "print(\"Text: \",text[0])\n",
    "print(\"Sentiment: \",sentiment[polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a62f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
